{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "344a55ee"
      ],
      "authorship_tag": "ABX9TyPoZU661cWdHaTtA6/KEJPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bariswheel/Tuning-DeepSeek-for-Diabetes/blob/main/Outputs_Cleared_REFACTORED_DeepSeek_Diabetes_CleanV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84e0a057"
      },
      "source": [
        "# ü©∫ DEEPSEEK R-1 Graph-RAG for Chinese Diabetes Guidelines ‚Äì End-to-End Pipeline (Refactored Translation & Evaluation)\n",
        "\n",
        "This notebook builds a **compact Retrieval-Augmented Generation (RAG) system** that turns 41 Chinese clinical-guideline JSON files into:\n",
        "\n",
        "1.  **Fully English knowledge triples** ‚Äì human-readable, one per FAISS vector, generated using a robust `deep_translator` process with caching and fallbacks.\n",
        "2.  **A FAISS flat-IP index** ‚Äì vectors encoded using `sentence-transformers` (MiniLM), ready for millisecond retrieval.\n",
        "3.  **A RAG demo** ‚Äì answering questions using a language model (DeepSeek-7B) augmented with retrieved knowledge graph triples.\n",
        "4.  **An evaluation** ‚Äì comparing model performance with and without RAG using keyword-based metrics.\n",
        "\n",
        "**Key Improvements and Features:**\n",
        "\n",
        "*   **Refactored Translation:** The translation process now uses the `deep_translator` library with Google Translate and MyMemory fallbacks, incorporating in-memory caching and retry logic with exponential backoff for improved speed and reliability, especially when dealing with API rate limits.\n",
        "*   **Parallel Processing:** Translation of JSON files is parallelized using `ThreadPoolExecutor` for faster execution.\n",
        "*   **Robust Setup:** Library installations are consolidated and managed to avoid dependency conflicts encountered during development.\n",
        "*   **Clear Pipeline:** The notebook follows a logical flow from data loading and translation to indexing, retrieval, RAG demonstration, and evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Notebook Execution Guide\n",
        "\n",
        "Follow these steps to run the entire pipeline from scratch:\n",
        "\n",
        "| Order | Cell ID | Purpose                                                                 | Notes                                                                 |\n",
        "| :---- | :------ | :---------------------------------------------------------------------- | :-------------------------------------------------------------------- |\n",
        "| 1     | Cell 0  | Mount Google Drive & Define Paths                                       | Ensures access to data and output directories.                        |\n",
        "| 2     | Cell 1  | Install Required Libraries                                              | Installs all necessary Python packages, including `deep_translator`.  |\n",
        "| 3     | Cell 3  | Define `safe_translate` Function                                        | Sets up the translation function with caching and fallbacks.          |\n",
        "| 4     | Cell 2  | Translate Raw CN JSON ‚Üí `*_en.json` (Parallel)                          | Translates source files. Skips if `*_en.json` files already exist.    |\n",
        "| 5     | Cell 4  | Build FAISS flat-IP index                                               | Embeds translated triples and creates the searchable index.         |\n",
        "| 6     | Cell 5  | Load FAISS index + encoder                                              | Loads the index and embedding model into memory for RAG.            |\n",
        "| 7     | Cell 6  | `retrieve_ctx()` helper + quick demo                                    | Defines and tests the function for retrieving context from the index. |\n",
        "| 8     | Cell 7  | 5-question RAG showcase (DeepSeek-7B)                                   | Demonstrates the RAG system answering questions.                      |\n",
        "| 9     | Eval 1  | Define Evaluation Data                                                  | Sets up test questions and ground truth answers.                      |\n",
        "| 10    | Eval 2  | Generate Answers Without RAG                                            | Gets baseline answers from the model (no RAG).                        |\n",
        "| 11    | Eval 3  | Generate Answers With RAG                                               | Gets answers from the RAG system (with retrieved context).            |\n",
        "| 12    | Eval 4  | Evaluate Answers and Calculate Scores                                   | Compares generated answers and calculates basic metrics.            |\n",
        "| Optional | Appx A1 | Analyze JSON Structure and Counts                                       | Provides insights into the translated data structure.               |\n",
        "| Optional | Appx A2 | Explanation of JSON Structure Analysis                                | Explains the analysis in Appx A1.                                     |\n",
        "\n",
        "**Note:** If we restart the runtime, we will need to re-run steps 1-8 (or 1-12 for full evaluation) as needed, starting with Cell 0. If translation is already done (Step 4), Cell 2 will skip quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogjMrufVOUk8"
      },
      "outputs": [],
      "source": [
        "# %% CELL 0 ‚Äì Mount Drive + paths\n",
        "\"\"\"Mount GDrive (silently re-uses an existing token) and define all\n",
        "folder / file constants in one place so later cells stay in sync.\"\"\"\n",
        "from google.colab import drive\n",
        "from pathlib import Path, PurePosixPath\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=False)    # one-liner mount\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/diakg_assets\")     # adjust once\n",
        "RAW_CN   = DATA_DIR / \"0521_new_format\"    # 41 raw CN guideline JSON\n",
        "JSON_DIR = DATA_DIR / \"diakg_en\"           # translated *_en.json (output)\n",
        "JSON_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "LABELS_CSV = DATA_DIR / \"entities_bilingual.csv\"   # built next cell\n",
        "SENT_PATH  = DATA_DIR / \"sentences.txt\"            # 1 line ‚âà 1 vector\n",
        "INDEX_PATH = DATA_DIR / \"graphrag_faiss.index\"     # 8 643 vec FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **What this cell does, step-by-step**  \n",
        "> 0 ¬∑ Mount GDrive so results survive runtime restarts.  \n",
        "> 1 ¬∑ Define *all* paths in **DATA_DIR** so later cells never hard-code strings.  \n",
        "> 2 ¬∑ Create `diakg_en/` if missing ‚Äì translated JSON will land there."
      ],
      "metadata": {
        "id": "cs2LvOFMPoD9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a23299c"
      },
      "source": [
        "# %% CELL 1 - INSTALL ALL REQUIRED LIBRARIES\n",
        "# Install deep_translator for the new translation method\n",
        "%pip install -q deep_translator\n",
        "\n",
        "# Install core libraries for embeddings and RAG\n",
        "# Pinning specific versions based on previous troubleshooting attempts\n",
        "# Let sentence-transformers manage its transformers and huggingface_hub dependencies\n",
        "%pip install -q --upgrade \\\n",
        "    sentence-transformers==2.7.0 \\\n",
        "    peft==0.10.0 \\\n",
        "    accelerate==0.26.1 \\\n",
        "    bitsandbytes==0.43.2\n",
        "\n",
        "# Note: We are NOT explicitly pinning transformers or huggingface_hub here,\n",
        "# letting sentence-transformers==2.7.0 find compatible versions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfc2f90c"
      },
      "source": [
        "# %% CELL 1.5 ‚Äì Define Translation Function\n",
        "import json\n",
        "import time\n",
        "import random # Import random for jitter in backoff\n",
        "from deep_translator import GoogleTranslator, MyMemoryTranslator, exceptions\n",
        "from pathlib import Path\n",
        "\n",
        "# Initialize translators\n",
        "# You might need to handle API keys if required by the services for higher usage\n",
        "google_translator = GoogleTranslator(source='zh-CN', target='en')\n",
        "mymemory_translator = MyMemoryTranslator(source='zh-CN', target='en-US')\n",
        "\n",
        "# Simple in-memory cache for translations\n",
        "translation_cache = {}\n",
        "\n",
        "def safe_translate(text: str, cache=translation_cache, max_retries=5) -> str:\n",
        "    \"\"\"\n",
        "    Translates text from Chinese to English using Google Translate,\n",
        "    with MyMemory as a fallback and simple caching.\n",
        "    Includes a retry mechanism with exponential backoff for rate limit errors.\n",
        "    \"\"\"\n",
        "    if not text or text.isspace():\n",
        "        return text # Return empty or whitespace strings as is\n",
        "\n",
        "    # Check cache first\n",
        "    if text in cache:\n",
        "        # print(f\"Cache hit for: {text[:50]}...\") # Optional: for debugging cache hits\n",
        "        return cache[text]\n",
        "\n",
        "    translated_text = None\n",
        "    attempt = 0\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            # Attempt translation with Google Translate\n",
        "            # print(f\"Attempt {attempt + 1}: Attempting Google Translate for: {text[:50]}...\") # Optional\n",
        "            translated_text = google_translator.translate(text)\n",
        "            if translated_text:\n",
        "                cache[text] = translated_text # Store in cache\n",
        "                return translated_text\n",
        "\n",
        "        except (exceptions.TooManyRequests, exceptions.APIException) as e:\n",
        "            # Catch rate limit or general API errors\n",
        "            print(f\"Attempt {attempt + 1}: Google Translate failed for '{text[:50]}...' due to API error: {e}\")\n",
        "            attempt += 1\n",
        "            if attempt < max_retries:\n",
        "                sleep_time = (2 ** attempt) + random.uniform(0, 1) # Exponential backoff with jitter\n",
        "                print(f\"Waiting {sleep_time:.2f} seconds before retrying...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                print(f\"Max retries ({max_retries}) reached for Google Translate.\")\n",
        "                break # Exit loop to try fallback\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected errors from Google Translate\n",
        "            print(f\"Attempt {attempt + 1}: Google Translate failed for '{text[:50]}...' due to unexpected error: {e}\")\n",
        "            attempt += 1\n",
        "            # For unexpected errors, maybe don't backoff aggressively, but still wait\n",
        "            time.sleep(1) # Wait for a fixed time\n",
        "            if attempt >= max_retries:\n",
        "                 print(f\"Max retries ({max_retries}) reached for Google Translate on unexpected error.\")\n",
        "                 break # Exit loop to try fallback\n",
        "\n",
        "\n",
        "    # If Google Translate failed after retries, attempt with MyMemory Translator\n",
        "    if not translated_text:\n",
        "        attempt = 0 # Reset attempt counter for fallback\n",
        "        while attempt < max_retries:\n",
        "            try:\n",
        "                 # print(f\"Attempt {attempt + 1}: Attempting MyMemory Translate fallback for: {text[:50]}...\") # Optional\n",
        "                 translated_text = mymemory_translator.translate(text)\n",
        "                 if translated_text:\n",
        "                     cache[text] = translated_text # Store in cache\n",
        "                     return translated_text\n",
        "\n",
        "            except (exceptions.TooManyRequests, exceptions.APIException) as e:\n",
        "                 # Catch rate limit or general API errors for fallback\n",
        "                 print(f\"Attempt {attempt + 1}: MyMemory Translate fallback failed for '{text[:50]}...' due to API error: {e}\")\n",
        "                 attempt += 1\n",
        "                 if attempt < max_retries:\n",
        "                      sleep_time = (2 ** attempt) + random.uniform(0, 1) # Exponential backoff with jitter\n",
        "                      print(f\"Waiting {sleep_time:.2f} seconds before retrying fallback...\")\n",
        "                      time.sleep(sleep_time)\n",
        "                 else:\n",
        "                      print(f\"Max retries ({max_retries}) reached for MyMemory Translate fallback.\")\n",
        "                      break # Exit loop\n",
        "\n",
        "            except Exception as e:\n",
        "                # Catch any other unexpected errors from MyMemory Translate\n",
        "                print(f\"Attempt {attempt + 1}: MyMemory Translate fallback failed for '{text[:50]}...' due to unexpected error: {e}\")\n",
        "                attempt += 1\n",
        "                time.sleep(1) # Wait for a fixed time\n",
        "                if attempt >= max_retries:\n",
        "                     print(f\"Max retries ({max_retries}) reached for MyMemory Translate fallback on unexpected error.\")\n",
        "                     break # Exit loop\n",
        "\n",
        "\n",
        "    # If both translators failed after retries\n",
        "    print(f\"Warning: Translation failed for text after multiple retries: '{text}'\")\n",
        "    return text # Return original text on complete failure\n",
        "\n",
        "print(\"‚úì Defined safe_translate function with caching, fallbacks, and retry with exponential backoff.\")\n",
        "\n",
        "# Example usage (optional, for testing the function)\n",
        "# print(\"\\nTesting translation function:\")\n",
        "# print(\"Hello (cached):\", safe_translate(\"‰Ω†Â•Ω\"))\n",
        "# print(\"Hello again (cache hit):\", safe_translate(\"‰Ω†Â•Ω\"))\n",
        "# print(\"Diabetes:\", safe_translate(\"Á≥ñÂ∞øÁóÖ\"))\n",
        "# print(\"Patient:\", safe_translate(\"ÊÇ£ËÄÖ\"))\n",
        "# # Example of a call that might fail (for testing backoff - uncomment and run multiple times if needed)\n",
        "# # print(\"Long text that might cause error:\", safe_translate(\"ËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÈïøÁöÑÂè•Â≠êÔºåÁî®‰∫éÊµãËØïÁøªËØëÂô®ÁöÑÈîôËØØÂ§ÑÁêÜÂíåÈáçËØïÊú∫Âà∂ÔºåÁúãÁúãÂÆÉÊòØÂê¶ËÉΩÂú®ÈÅáÂà∞ÈóÆÈ¢òÊó∂Ê≠£Á°ÆÂú∞Á≠âÂæÖÂíåÈáçËØï„ÄÇ\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 2 ‚Äì Translate raw CN JSON ‚Üí *_en.json (using deep_translator and ThreadPoolExecutor)\n",
        "\"\"\"\n",
        "Translate raw Chinese JSON files into English JSON files (*_en.json) in parallel\n",
        "using the safe_translate function with caching and fallbacks.\n",
        "This replaces the previous NLLB-200 based translation.\n",
        "\"\"\"\n",
        "import json\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "# Removed sys as we are no longer using sys.exit()\n",
        "# import sys\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Assuming DATA_DIR and RAW_CN are defined in Cell 0\n",
        "# Assuming JSON_DIR is defined in Cell 0 and exists\n",
        "# Assuming safe_translate function and translation_cache are defined in Cell 3\n",
        "\n",
        "# Ensure JSON_DIR exists\n",
        "JSON_DIR = DATA_DIR / \"diakg_en\"\n",
        "JSON_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "raw_files = list(RAW_CN.glob(\"*.json\"))\n",
        "translated_files = list(JSON_DIR.glob(\"*_en.json\"))\n",
        "\n",
        "# Check if translation is already done (based on presence of _en.json files)\n",
        "# Use an if/else block to skip translation instead of sys.exit()\n",
        "if len(raw_files) > 0 and len(translated_files) >= len(raw_files):\n",
        "    print(\"**All raw JSON files appear to have been translated. Skipping translation.**\")\n",
        "    # The cell will simply finish execution here without the translation loop\n",
        "else:\n",
        "    print(f\"Translating {len(raw_files)} raw JSON files from {RAW_CN} to {JSON_DIR} using ThreadPoolExecutor...\")\n",
        "\n",
        "    # --- Function to translate a single file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    def translate_file(jp):\n",
        "        try:\n",
        "            with open(jp, encoding=\"utf8\") as f:\n",
        "                doc = json.load(f)\n",
        "\n",
        "            # Iterate through paragraphs, sentences, entities, and relations to translate text fields\n",
        "            if \"paragraphs\" in doc:\n",
        "                for p in doc[\"paragraphs\"]:\n",
        "                    if \"sentences\" in p:\n",
        "                        for s in p[\"sentences\"]:\n",
        "                            # Translate sentence text\n",
        "                            zh_sent = s.get(\"sentence\", \"\")\n",
        "                            if zh_sent:\n",
        "                                s[\"sentence_en\"] = safe_translate(zh_sent)\n",
        "\n",
        "                            # Translate entity labels\n",
        "                            if \"entities\" in s:\n",
        "                                for ent in s[\"entities\"]:\n",
        "                                    zh_lab  = ent.get(\"entity\", \"\")\n",
        "                                    if zh_lab:\n",
        "                                        ent[\"entity_en\"] = safe_translate(zh_lab)\n",
        "\n",
        "                            # Translate relation types\n",
        "                            if \"relations\" in s:\n",
        "                                for r in s.get(\"relations\", []):\n",
        "                                    zh_rel = r.get(\"relation_type\", \"\")\n",
        "                                    if zh_rel:\n",
        "                                        r[\"relation_en\"] = safe_translate(zh_rel)\n",
        "\n",
        "\n",
        "            # Determine output filename and save the translated document\n",
        "            out_name = os.path.basename(jp).replace(\".json\", \"_en.json\")\n",
        "            out_path = JSON_DIR / out_name\n",
        "\n",
        "            with open(out_path, \"w\", encoding=\"utf8\") as f:\n",
        "                json.dump(doc, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Print error without interrupting the ThreadPoolExecutor\n",
        "            print(f\"\\nError processing file {jp}: {e}\", file=sys.stderr)\n",
        "            # Return an indicator of failure\n",
        "            return False\n",
        "        # Return an indicator of success\n",
        "        return True\n",
        "\n",
        "    # --- Parallel translation pass ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    # Use ThreadPoolExecutor for parallel processing\n",
        "    # Adjust max_workers based on the Colab runtime's CPU cores and memory\n",
        "    # A good starting point is often the number of CPU cores or slightly more\n",
        "    num_workers = os.cpu_count() # Get the number of CPU cores\n",
        "    if num_workers is None:\n",
        "        num_workers = 4 # Default to 4 if not detectable or needed\n",
        "\n",
        "    print(f\"Using {num_workers} worker threads for translation...\")\n",
        "\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Map the translate_file function to each raw JSON file\n",
        "        # Use list() to trigger execution and wait for results\n",
        "        # tqdm can be used to show progress for the results list\n",
        "        results = list(tqdm.tqdm(executor.map(translate_file, raw_files), total=len(raw_files), desc=\"Translating docs\"))\n",
        "\n",
        "    # Optional: Check results to see if any files failed to translate\n",
        "    # Need to re-import sys for stderr if it was removed above\n",
        "    import sys\n",
        "    failed_files = [raw_files[i] for i, success in enumerate(results) if not success]\n",
        "    if failed_files:\n",
        "        print(f\"\\nWarning: Failed to translate {len(failed_files)} files:\", file=sys.stderr)\n",
        "        for f in failed_files:\n",
        "            print(f\"- {f}\", file=sys.stderr)\n",
        "\n",
        "    print(\"‚úì Translation pass complete ‚Äî all *_en.json written (or skipped if already present).\")\n",
        "\n",
        "# If the translation was skipped, this final print will still run\n",
        "# print(\"‚úì Translation check complete.\") # Optional: add a final print outside the else"
      ],
      "metadata": {
        "id": "ywwXFaAmRgJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 3 ‚Äì rebuild sentences.txt (force=True)\n",
        "\"\"\"Scan every *_en.json* and write one English triple string per KG triple\n",
        "(head_en relation_en tail_en) ‚Äì 8 643 lines in sample run.\n",
        "Now reads translated entity and relation labels directly from *_en.json files.\"\"\"\n",
        "\n",
        "\"\"\"In other words, This cell takes the translated JSON, finds all the head-relation-tail\n",
        " onnections (triples), and writes each one as a single line in sentences.txt. When we\n",
        " tested this, we got 8,643 lines (triples) in that file.\"\"\"\n",
        "\n",
        "import json, glob, pandas as pd, tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "FORCE = False                              # flip to False once file is stable\n",
        "\n",
        "if FORCE or not SENT_PATH.exists():\n",
        "    print(\"‚ü≥ Rebuilding sentences.txt ‚Ä¶\")\n",
        "    # The old logic read from entities_bilingual.csv\n",
        "    # label_map = pd.read_csv(LABELS_CSV).set_index(\"entity_id\")[\"en_label\"].to_dict()\n",
        "\n",
        "    sentences = []\n",
        "    for jp in tqdm.tqdm(glob.glob(str(JSON_DIR / \"*_en.json\")), desc=\"Scanning JSON\"):\n",
        "        doc = json.load(open(jp, encoding=\"utf8\"))\n",
        "        if \"paragraphs\" in doc:\n",
        "            for p in doc[\"paragraphs\"]:\n",
        "                if \"sentences\" in p:\n",
        "                    # Build a temporary entity ID to English label map for this sentence\n",
        "                    # This is needed because relations store entity IDs, not the labels directly\n",
        "                    entity_map = {ent.get(\"entity_id\"): ent.get(\"entity_en\", ent.get(\"entity\", \"\"))\n",
        "                                  for sent in p.get(\"sentences\", []) # Iterate over sentences in the paragraph\n",
        "                                  for ent in sent.get(\"entities\", []) # Iterate over entities in each sentence\n",
        "                                  if ent.get(\"entity_id\")} # Only include if entity_id is present\n",
        "\n",
        "\n",
        "                    for s in p[\"sentences\"]:\n",
        "                        if \"relations\" in s:\n",
        "                            for r in s.get(\"relations\", []):\n",
        "                                head_id = r.get('head_entity_id')\n",
        "                                tail_id = r.get('tail_entity_id')\n",
        "                                relation_en = r.get('relation_en', r.get('relation_type', '')) # Use relation_en, fallback to relation_type\n",
        "\n",
        "                                # Get the translated head and tail entity labels using the map\n",
        "                                head_en = entity_map.get(head_id, head_id) # Fallback to ID if label not found\n",
        "                                tail_en = entity_map.get(tail_id, tail_id) # Fallback to ID if label not found\n",
        "\n",
        "                                # Ensure we have valid head, relation, and tail before adding\n",
        "                                if head_en and relation_en and tail_en:\n",
        "                                    sentences.append(\n",
        "                                        f\"{head_en} {relation_en} {tail_en}\"\n",
        "                                    )\n",
        "\n",
        "    SENT_PATH.write_text(\"\\n\".join(sentences))\n",
        "    print(\"‚úî sentences.txt rebuilt with\", len(sentences), \"lines\")\n",
        "else:\n",
        "    print(\"‚úì sentences.txt already present ‚Äì nothing to do.\")"
      ],
      "metadata": {
        "id": "yjErxWrYR94I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Summary**\n",
        "\n",
        "In summary, this cell's primary function is to generate a text file containing all the knowledge graph triples in a human-readable English format, using the entity label mapping created in Cell 1. The FORCE flag provides control over whether to regenerate this file, making subsequent runs faster if the data hasn't changed.\n",
        "\n",
        "> **What this cell does, step-by-step**  \n",
        "> 1 ¬∑ Optionally force-delete the old file (`FORCE=True`) so we overwrite the ID-only version.  \n",
        "> 2 ¬∑ Walk every *_en.json* and concatenate **head EN + relation_en + tail EN**.  \n",
        "> 3 ¬∑ Write one line per triple to **sentences.txt** ‚Äì the order matches the FAISS index we will build later."
      ],
      "metadata": {
        "id": "xVrKhahsyOzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Updates for optimization ‚Äî **Cell 2: Translate raw CN JSON ‚Üí *_en.json***\n",
        "\n",
        "| What we changed | Why it matters on the A100 |\n",
        "|-----------------|----------------------------|\n",
        "| **Early-exit guard**<br>`if (DATA_DIR/\"diakg_en\").exists() and (DATA_DIR/\"entities_bilingual.csv\").exists():` | We skip the two-hour translation pass on every warm run. |\n",
        "| **Single `label_map` load** | All CN ‚Üí EN look-ups happen in-memory; no extra CSV I/O. |\n",
        "| **Reuse the translator object that is already on GPU** | We avoid re-loading NLLB-200 and keep VRAM steady. |\n",
        "| **Translate both in-sentence & top-level `relation_type` strings** | Removes the last source of ‚ÄúT1234 Test_Disease ‚Ä¶‚Äù artefacts later in retrieval. |\n",
        "| **Optional tip:** raise `max_length` from `400` ‚Üí `512` in `fast_translate()` | Silences the ‚Äúinput_length > 0.9 √ó max_length‚Äù warnings and speeds up long sentences (fits comfortably in 16 GB fp16). |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Updates for optimization ‚Äî **Cell 3: rebuild `sentences.txt`**\n",
        "\n",
        "| What we changed | Why it matters on the A100 |\n",
        "|-----------------|----------------------------|\n",
        "| **`FORCE = False` by default** | We rebuild only when the file is missing or when we deliberately flip the flag, so most runs take < 1 s. |\n",
        "| **ID ‚Üí label mapping uses the CSV we just built** | Ensures every triple line is now fully human-readable (no more ID placeholders). |\n",
        "| **Streamlined triple writer**<br>`head_en  relation_en  tail_en` | Keeps one-line-per-vector order -> perfectly aligns with the FAISS vectors we embed in Cell 4 for fast retrieval. |\n",
        "\n",
        "> **Workflow tip**  \n",
        "> 1. If we ingest **new** CN JSON later, delete `entities_bilingual.csv` **or** set `FORCE = True` once.  \n",
        "> 2. Run Cells 0 ‚Üí 3 ‚Äî only the new data is processed.  \n",
        "> 3. Flip `FORCE` back to `False` and enjoy instant warm starts."
      ],
      "metadata": {
        "id": "_jwGpKTl5Ajt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 4 ‚Äì Build FAISS flat-IP index\n",
        "\"\"\"Embed every sentence line with MiniLM-L6-v2 (384-d) ‚Üí save indexFlatIP.\"\"\"\n",
        "\n",
        "import faiss, numpy as np, tqdm, torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentences = [ln.rstrip() for ln in open(SENT_PATH)]\n",
        "print(\"Total triples:\", len(sentences))\n",
        "\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                               device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vecs = embedder.encode(sentences, batch_size=512,  # A100 handles 512 easily\n",
        "                       convert_to_numpy=True, show_progress_bar=True).astype(\"float32\")\n",
        "\n",
        "index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "index.add(vecs)\n",
        "faiss.write_index(index, str(INDEX_PATH))\n",
        "print(f\"‚úî FAISS index written with {index.ntotal} vectors\")"
      ],
      "metadata": {
        "id": "9ok1E-q9yPbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Summary of what CELL 4 does**  \n",
        "> 1 ¬∑ Read **8 643** human-readable triples ‚Üí list[str].  \n",
        "> 2 ¬∑ Encode with MiniLM-L6-v2 in batches of **512** (A100 keeps <2 GB VRAM).  \n",
        "> 3 ¬∑ Add vectors to **IndexFlatIP** (inner-product ‚âà cosine on unit vecs).  \n",
        "> 4 ¬∑ Save **graphrag_faiss.index** ‚Äì ~12 MB on disk."
      ],
      "metadata": {
        "id": "kSSzrphyzSUy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5b85169"
      },
      "source": [
        "### What cell 4 does, step-by-step\n",
        "\n",
        "The purpose of this cell, as indicated by the docstring (lines 2-3), is to embed the English triple sentences using a sentence transformer model and then build and save a FAISS index of these embeddings.\n",
        "\n",
        "1.  **Load Sentences (line 7):**\n",
        "    *   Line 7 reads the `sentences.txt` file (created in Cell 3) and stores each line (which represents a knowledge graph triple) as an element in a Python list called `sentences`. The `rstrip()` method is used to remove any trailing whitespace, including the newline character.\n",
        "    *   Line 8 prints the total number of triples loaded from the file.\n",
        "\n",
        "2.  **Load Sentence Embedder (lines 10-12):**\n",
        "    *   Line 10 initializes a `SentenceTransformer` model. We are using the \"sentence-transformers/all-MiniLM-L6-v2\" model, which is a pre-trained model optimized for generating sentence embeddings.\n",
        "    *   Line 11 specifies the device to load the model onto. It checks if a CUDA-enabled GPU is available and uses `'cuda'` if it is, otherwise it uses `'cpu'`. This ensures the embedding process can leverage GPU acceleration if available.\n",
        "\n",
        "3.  **Encode Sentences (lines 13-14):**\n",
        "    *   Line 13 uses the loaded `embedder` model to encode the list of `sentences` into numerical vectors.\n",
        "        *   `batch_size=512`: This processes the sentences in batches of 512, which is efficient for GPU processing, especially on an A100 as noted in the comment.\n",
        "        *   `convert_to_numpy=True`: This converts the output PyTorch tensors to NumPy arrays.\n",
        "        *   `show_progress_bar=True`: This displays a progress bar during the encoding process, which can take some time for a large number of sentences.\n",
        "    *   Line 14 converts the data type of the resulting vectors (`vecs`) to `float32`, which is a common and efficient format for FAISS.\n",
        "\n",
        "4.  **Build and Add to FAISS Index (lines 16-17):**\n",
        "    *   Line 16 initializes a FAISS index. `faiss.IndexFlatIP` creates a flat index that uses the inner product (IP) for similarity search. Inner product is equivalent to cosine similarity when the vectors are normalized to unit length, which sentence transformers typically produce. `vecs.shape[1]` provides the dimensionality of the vectors (384 for MiniLM-L6-v2).\n",
        "    *   Line 17 adds the computed vectors (`vecs`) to the FAISS index.\n",
        "\n",
        "5.  **Save FAISS Index (line 18):**\n",
        "    *   Line 18 saves the built FAISS index to a file specified by `INDEX_PATH`. FAISS provides optimized functions for writing indexes to disk.\n",
        "\n",
        "6.  **Print Confirmation (line 19):**\n",
        "    *   Line 19 prints a confirmation message indicating that the FAISS index has been written and shows the total number of vectors added to the index (`index.ntotal`).\n",
        "\n",
        "In essence, this cell takes the English triples, converts them into numerical representations (vectors) using a sentence transformer, and then organizes these vectors into a searchable FAISS index for efficient retrieval."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 5 ‚Äì Load FAISS index + encoder\n",
        "\"\"\"Bring the EN vectors + MiniLM encoder into memory for retrieval.\"\"\"\n",
        "\n",
        "import faiss, torch, numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "index     = faiss.read_index(str(INDEX_PATH))\n",
        "embedder  = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "with open(SENT_PATH) as f:\n",
        "    sentences = [ln.rstrip() for ln in f]\n",
        "\n",
        "print(f\"‚úì FAISS index with {index.ntotal:,} English vectors loaded\")"
      ],
      "metadata": {
        "id": "61xtyIkFzSz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **What this cell does, step-by-step**  \n",
        "> 0 ¬∑ Read **graphrag_faiss.index** (8 643 vec) ‚Äì 200 ms on SSD.  \n",
        "> 1 ¬∑ Load the *same* MiniLM encoder used in Cell 4 so query / index live in the same space.  \n",
        "> 2 ¬∑ Read **sentences.txt** into a Python list ‚Üí keeps vector ‚Üî text mapping handy."
      ],
      "metadata": {
        "id": "QGAYOJ6ozgbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 6 ‚Äì retrieve_ctx() + quick demo, the R in RAG\n",
        "\n",
        "import numpy as np, torch\n",
        "\n",
        "def retrieve_ctx(question: str, k: int = 5):\n",
        "    qvec = embedder.encode([question]).astype(\"float32\")\n",
        "    D, I = index.search(qvec, k*3)          # ask for a few extra\n",
        "    seen, ctx = set(), []\n",
        "    for i, s in zip(I[0], D[0]):\n",
        "        triple = sentences[i]\n",
        "        if triple not in seen:              # keep only the first copy\n",
        "            ctx.append({\"triple\": triple, \"score\": float(s)})\n",
        "            seen.add(triple)\n",
        "        if len(ctx) == k:                   # stop once we have k uniques\n",
        "            break\n",
        "    return ctx\n",
        "\n",
        "# quick demo\n",
        "question = \"Which drug treats type 2 diabetes?\"\n",
        "ctx = retrieve_ctx(question)\n",
        "print(\"Q:\", question)\n",
        "for c in ctx:\n",
        "    print(\" ‚Ä¢\", c[\"triple\"], f\"(score {c['score']:.3f})\")"
      ],
      "metadata": {
        "id": "NB2QchE7zg3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What this cell does, step-by-step  \n",
        "0. **Inputs in RAM**  \n",
        "   * `index`‚ÄÉ‚Üí FAISS flat-IP index loaded in Cell 5.  \n",
        "   * `embedder` ‚Üí MiniLM-L6 encoder (384-d) loaded in Cell 5.  \n",
        "   * `sentences` ‚Üí Python list that maps every vector to its triple string.  \n",
        "\n",
        "1. **Encode the user question**  \n",
        "   * We run the question through **MiniLM-L6** ‚Üí 384-d query vector (`qvec`) on GPU.\n",
        "\n",
        "2. **Search FAISS for nearest neighbours**  \n",
        "   * We ask for **k √ó 3** vectors (default `k = 5` ‚Üí 15 hits).  \n",
        "   * Distance used is dot product or also called *inner-product* ‚Üí cosine on unit vectors.\n",
        "\n",
        "3. **Uniqueness filter (NEW)**  \n",
        "   * We iterate through the 15 hits **in order of similarity**.  \n",
        "   * We keep the first time we see a triple string and discard duplicates.  \n",
        "   * We stop as soon as we have **k unique triples** ‚Äì fast and deterministic.\n",
        "\n",
        "4. **Return a tidy Python list**  \n",
        "   * Each element is a dict `{\"triple\": str, \"score\": float}`.  \n",
        "   * Scores are FAISS inner-product values cast to `float` for JSON-friendliness.\n",
        "\n",
        "5. **Smoke-test**  \n",
        "   * The demo encodes *‚ÄúWhich drug treats type 2 diabetes?‚Äù* and prints the top-k unique triples with their scores.  \n",
        "   * On an A100 the end-to-end latency is **‚â™ 100 ms** per query.\n",
        "\n",
        "> **Why it matters** ‚Äì removing duplicates avoids five identical rows in the context block and slightly improves recall of diverse evidence for the LLM prompt."
      ],
      "metadata": {
        "id": "alx8jXZB1f4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% CELL 7 ‚Äì 5-question RAG showcase  (DeepSeek-7B in 8-bit)\n",
        "\"\"\"\n",
        "1. For each demo question we ‚Ä¶\n",
        "   ‚Ä¢ retrieve_ctx(k=5)                  ‚Üí 5 nearest triples\n",
        "   ‚Ä¢ build a prompt  (Context\\n‚Ä¶ triple‚Ä¶) + Q + \"Answer:\"\n",
        "   ‚Ä¢ call DeepSeek-7B (deterministic, temp=0) for the answer\n",
        "   ‚Ä¢ pretty-print everything (Q / A / top-k triples) for a nice screenshot.\n",
        "   ‚Äì deterministic output is great for grading / reproducibility.\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# 8-bit still fits easily on the A100 (‚âà40 GB VRAM)\n",
        "mdl = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    # load_in_8bit=True, # Removed to bypass bitsandbytes issue\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ").eval()\n",
        "\n",
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=mdl,\n",
        "    tokenizer=tok,\n",
        "    # Removed generation args from pipeline initialization\n",
        "    # max_new_tokens=128,\n",
        "    # temperature=0.0,\n",
        "    # do_sample=False,\n",
        ")\n",
        "\n",
        "print(\"‚úì DeepSeek-7B loaded (without 8-bit quantization)\")\n",
        "\n",
        "PROMPT_TMPL = \"\"\"You are a helpful medical assistant.\n",
        "Context:\n",
        "{ctx}\n",
        "\n",
        "Question: {q}\n",
        "Answer:\"\"\"\n",
        "\n",
        "questions = [\n",
        "    \"Which drug is prescribed to manage HbA1c in type 2 diabetes patients?\",\n",
        "    \"What long-term complication of diabetes affects the eyes?\",\n",
        "    \"Name one lifestyle change that reduces insulin resistance.\",\n",
        "    \"Which lab test is used to diagnose gestational diabetes?\",\n",
        "    \"How does metformin lower blood glucose mechanistically?\",\n",
        "]\n",
        "\n",
        "for qi, q in enumerate(questions, 1):\n",
        "    ctx  = retrieve_ctx(q, k=5)                       # step 1\n",
        "    ctx_block = \"\\n- \" + \"\\n- \".join(c[\"triple\"] for c in ctx)\n",
        "    prompt = PROMPT_TMPL.format(ctx=ctx_block, q=q)   # step 2\n",
        "\n",
        "    ans = gen(\n",
        "        prompt,\n",
        "        max_new_tokens=128,  # Pass generation args directly\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "    )[0][\"generated_text\"].split(\"Answer:\")[-1].strip()   # step 3\n",
        "\n",
        "    print(f\"\\n‚Äî‚Äî‚Äî  Q{qi} ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\")\n",
        "    print(\"Q:\", q)\n",
        "    print(\"A:\", ans)\n",
        "    print(\"‚Äî top-k context ‚Äî\")\n",
        "    for c in ctx:\n",
        "        print(\" ‚Ä¢\", c[\"triple\"], f\"(score {c['score']:.3f})\")             # step 4"
      ],
      "metadata": {
        "id": "IR0TplWUz5Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d096188"
      },
      "source": [
        "# Reinstall bitsandbytes with a specific configuration for the evaluation steps below\n",
        "%pip install -qq --upgrade bitsandbytes==0.43.2 --extra-index-url https://download.pytorch.org/whl/cu121"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3Jrwir0D7HIy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69154353"
      },
      "source": [
        "#### Explanation of EVALUATION STEP 1\n",
        "\n",
        "This code cell (lines 1-28) defines the evaluation data that we will use to test the RAG system.\n",
        "\n",
        "*   **Line 1:** This is a markdown heading indicating the start of the first evaluation step.\n",
        "*   **Lines 2-5:** This is a docstring that briefly explains the purpose of the cell.\n",
        "*   **Lines 8-18:** A Python list named `test_questions` is created. This list contains the questions that we will use to query both the \"with RAG\" and \"without RAG\" versions of the system.\n",
        "*   **Lines 21-28:** A Python dictionary named `ground_truth_answers` is created. The keys of this dictionary are the questions from the `test_questions` list, and the values are the expected correct answers. These ground truth answers are what we will compare the model's generated answers against to calculate evaluation scores. It is noted that these answers should ideally be based on the source guideline documents.\n",
        "*   **Line 30:** A print statement confirms the number of test questions and ground truth answers that have been defined.\n",
        "\n",
        "Defining these questions and answers upfront ensures that we are evaluating both RAG and non-RAG systems on the same set of inputs and have a standard to measure correctness against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a847179"
      },
      "source": [
        "# %% EVALUATION STEP 1 ‚Äì Define Evaluation Data\n",
        "\"\"\"\n",
        "This cell defines a set of test questions and their corresponding ground truth\n",
        "answers to be used for evaluating the RAG system's performance.\n",
        "\"\"\"\n",
        "\n",
        "# Define a list of test questions\n",
        "test_questions = [\n",
        "    \"Which drug is prescribed to manage HbA1c in type 2 diabetes patients?\",\n",
        "    \"What long-term complication of diabetes affects the eyes?\",\n",
        "    \"Name one lifestyle change that reduces insulin resistance.\",\n",
        "    \"Which lab test is used to diagnose gestational diabetes?\",\n",
        "    \"How does metformin lower blood glucose mechanistically?\",\n",
        "    \"What is a common side effect of SGLT2 inhibitors?\",\n",
        "    \"Which type of diabetes is characterized by insulin deficiency?\",\n",
        "    \"What is the target HbA1c level for most adults with diabetes?\",\n",
        "    \"Name a class of oral medications for type 2 diabetes.\",\n",
        "    \"What is the primary goal of diabetes management?\"\n",
        "]\n",
        "\n",
        "# Define the ground truth answers for the test questions\n",
        "# These should ideally be derived from the source documents (Chinese guidelines)\n",
        "ground_truth_answers = {\n",
        "    \"Which drug is prescribed to manage HbA1c in type 2 diabetes patients?\": \"Insulin or oral antidiabetic agents like Metformin, Sulfonylureas, etc.\",\n",
        "    \"What long-term complication of diabetes affects the eyes?\": \"Diabetic retinopathy\",\n",
        "    \"Name one lifestyle change that reduces insulin resistance.\": \"Weight loss, regular exercise, or a balanced diet.\",\n",
        "    \"Which lab test is used to diagnose gestational diabetes?\": \"Oral glucose tolerance test (OGTT)\",\n",
        "    \"How does metformin lower blood glucose mechanistically?\": \"Decreases hepatic glucose production and increases insulin sensitivity.\",\n",
        "    \"What is a common side effect of SGLT2 inhibitors?\": \"Genital mycotic infections or urinary tract infections.\",\n",
        "    \"Which type of diabetes is characterized by insulin deficiency?\": \"Type 1 diabetes\",\n",
        "    \"What is the target HbA1c level for most adults with diabetes?\": \"Typically <7% (53 mmol/mol), but individualized.\",\n",
        "    \"Name a class of oral medications for type 2 diabetes.\": \"Metformin, Sulfonylureas, Thiazolidinediones, DPP-4 inhibitors, SGLT2 inhibitors, GLP-1 receptor agonists (oral form).\",\n",
        "    \"What is the primary goal of diabetes management?\": \"Preventing acute complications and reducing the risk of long-term complications.\"\n",
        "}\n",
        "\n",
        "print(f\"Defined {len(test_questions)} test questions and ground truth answers.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4731f2b9"
      },
      "source": [
        "### Explanation of EVALUATION STEP 2\n",
        "\n",
        "This code cell (lines 1-26) generates responses to the `test_questions` using the DeepSeek-7B model *without* incorporating any context from the knowledge graph.\n",
        "\n",
        "*   **Lines 1-4:** This is the cell heading and docstring, outlining the cell's purpose.\n",
        "*   **Lines 6-7:** These lines assume that the `gen` pipeline (for the DeepSeek-7B model) and the `test_questions` list have been successfully loaded or defined in previous cells.\n",
        "*   **Line 9:** An empty dictionary `answers_without_rag` is initialized to store the generated answers, with questions as keys and answers as values.\n",
        "*   **Line 11:** A message is printed to indicate the start of the answer generation process.\n",
        "*   **Line 13:** A loop iterates through each `question` in the `test_questions` list.\n",
        "*   **Line 15:** The `prompt` for the model is simply the `question` itself, as we are not using RAG in this step.\n",
        "*   **Lines 17-22:** The `gen` pipeline is called with the `prompt`.\n",
        "    *   `max_new_tokens=128` limits the length of the generated response.\n",
        "    *   `temperature=0.0` and `do_sample=False` are set to make the model's output deterministic, which is helpful for evaluation consistency.\n",
        "*   **Line 25:** The generated text is extracted from the pipeline's output and leading/trailing whitespace is removed using `.strip()`. Note that depending on the model's output format, we might need to adjust how the answer is extracted if the model includes the original prompt or other conversational elements in the `generated_text`.\n",
        "*   **Line 28:** The extracted `generated_text` is stored in the `answers_without_rag` dictionary, keyed by the original `question`.\n",
        "*   **Lines 30-32:** The original question and the generated answer are printed for review.\n",
        "*   **Line 34:** A final message confirms that the process is complete.\n",
        "\n",
        "This step establishes a baseline by showing how well the model performs on the questions using only its pre-trained knowledge, before we introduce the retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc6bcaf3",
        "collapsed": true
      },
      "source": [
        "# %% EVALUATION STEP 2 ‚Äì Generate Answers Without RAG\n",
        "\"\"\"\n",
        "This cell generates answers to the test questions using the DeepSeek-7B model\n",
        "without providing any external retrieved context.\n",
        "\"\"\"\n",
        "\n",
        "# Assuming the DeepSeek-7B pipeline 'gen' and 'test_questions' are defined in previous cells\n",
        "\n",
        "answers_without_rag = {}\n",
        "\n",
        "print(\"Generating answers without RAG...\")\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    # Prompt the model with just the question\n",
        "    prompt = question\n",
        "    # Use the text generation pipeline\n",
        "    response = gen(\n",
        "        prompt,\n",
        "        max_new_tokens=128,  # Limit the response length\n",
        "        temperature=0.0,     # Use temperature 0 for deterministic output\n",
        "        do_sample=False      # Do not use sampling\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    # The response format might vary slightly based on the model/pipeline,\n",
        "    # we'll take the full generated text here for simplicity.\n",
        "    generated_text = response[0]['generated_text'].strip()\n",
        "\n",
        "    # Store the generated answer\n",
        "    answers_without_rag[question] = generated_text\n",
        "\n",
        "    print(f\"Q{i+1}: {question}\")\n",
        "    print(f\"A (without RAG): {generated_text}\\n\")\n",
        "\n",
        "print(\"Finished generating answers without RAG.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "711f42c4"
      },
      "source": [
        "### Explanation of EVALUATION STEP 3\n",
        "\n",
        "This code cell (lines 1-39) generates responses to the `test_questions` using the DeepSeek-7B model, this time incorporating retrieved context from our knowledge graph.\n",
        "\n",
        "*   **Lines 1-4:** This is the cell heading and docstring, outlining the cell's purpose.\n",
        "*   **Lines 6-7:** These lines assume that the `gen` pipeline, the `test_questions` list, and the `retrieve_ctx` function have been successfully loaded or defined in previous cells.\n",
        "*   **Line 9:** An empty dictionary `answers_with_rag` is initialized to store the generated answers when using RAG.\n",
        "*   **Line 10:** An optional empty dictionary `retrieved_contexts` is initialized to store the context retrieved for each question, which can be useful for debugging or analysis.\n",
        "*   **Line 12:** A message is printed to indicate the start of the answer generation process with RAG.\n",
        "*   **Line 14:** A loop iterates through each `question` in the `test_questions` list.\n",
        "*   **Lines 16-18:** The `retrieve_ctx` function is called with the current `question` and a specified number of results (`k=5`). The returned list of context triples is stored in the `context` variable and optionally saved in `retrieved_contexts`.\n",
        "*   **Lines 21-23:** The retrieved `context` (a list of dictionaries) is formatted into a single string `ctx_block` suitable for inclusion in the prompt, using a similar format to the demo in Cell 7.\n",
        "*   **Lines 26-28:** The full `prompt` for the language model is constructed using the `PROMPT_TMPL` (defined in Cell 7), inserting the formatted `ctx_block` and the original `question`.\n",
        "*   **Lines 31-36:** The `gen` pipeline is called with the RAG-augmented `prompt`. Similar generation parameters (`max_new_tokens`, `temperature`, `do_sample`) are used as in Evaluation Step 2 and Cell 7 to ensure deterministic output and limit length.\n",
        "*   **Line 39:** The generated text is extracted from the pipeline's output, splitting by \"Answer:\" and taking the part after it, then stripping whitespace.\n",
        "*   **Line 42:** The extracted `generated_text` is stored in the `answers_with_rag` dictionary, keyed by the original `question`.\n",
        "*   **Lines 44-46:** The original question and the generated answer (with RAG) are printed for review. Optional lines (commented out) are included to print the retrieved context as well.\n",
        "*   **Line 52:** A final message confirms that the process is complete.\n",
        "\n",
        "This step generates the answers that leverage the knowledge graph, allowing us to compare them directly to the baseline answers generated without RAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "454f792c",
        "collapsed": true
      },
      "source": [
        "# %% EVALUATION STEP 3 ‚Äì Generate Answers With RAG\n",
        "\"\"\"\n",
        "This cell generates answers to the test questions using the DeepSeek-7B model\n",
        "by providing retrieved context from the knowledge graph.\n",
        "\"\"\"\n",
        "\n",
        "# Assuming 'gen', 'test_questions', 'retrieve_ctx', and 'tok' are defined in previous cells\n",
        "\n",
        "answers_with_rag = {}\n",
        "retrieved_contexts = {} # Optional: store retrieved contexts for review\n",
        "\n",
        "print(\"Generating answers with RAG...\")\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    # Retrieve context for the question\n",
        "    # We'll use k=5 as in the Cell 7 demo, but this can be adjusted\n",
        "    context = retrieve_ctx(question, k=5)\n",
        "    retrieved_contexts[question] = context # Store context\n",
        "\n",
        "    # Format the context for the prompt\n",
        "    # Using the same format as in Cell 7\n",
        "    ctx_block = \"\\n- \" + \"\\n- \".join(c[\"triple\"] for c in context)\n",
        "\n",
        "    # Build the prompt with the retrieved context\n",
        "    # Using the same prompt template as in Cell 7\n",
        "    prompt = PROMPT_TMPL.format(ctx=ctx_block, q=question)\n",
        "\n",
        "    # Calculate the input length after tokenization\n",
        "    input_ids = tok(prompt, return_tensors=\"pt\").input_ids\n",
        "    input_length = input_ids.shape[-1]\n",
        "\n",
        "    # Use the text generation pipeline with the RAG prompt\n",
        "    # Pass generation arguments directly as keyword arguments to the pipeline call\n",
        "    response = gen(\n",
        "        prompt,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "        max_length=input_length + 128 # Explicitly set max_length\n",
        "    )\n",
        "\n",
        "    # Extract the generated text, similar to Cell 7\n",
        "    # We split by \"Answer:\" and take the part after it\n",
        "    generated_text = response[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Store the generated answer\n",
        "    answers_with_rag[question] = generated_text\n",
        "\n",
        "    print(f\"Q{i+1}: {question}\")\n",
        "    print(f\"A (with RAG): {generated_text}\\n\")\n",
        "    # Optional: print retrieved context for debugging/review\n",
        "    # print(\"Retrieved Context:\")\n",
        "    # for c in context:\n",
        "    #     print(f\"  - {c['triple']} (score: {c['score']:.3f})\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "\n",
        "print(\"Finished generating answers with RAG.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff91b00a"
      },
      "source": [
        "### Explanation of EVALUATION STEP 4\n",
        "\n",
        "This code cell (lines 1-81) performs a basic evaluation of the generated answers by comparing them to the ground truth answers and calculating accuracy, precision, and F1-score.\n",
        "\n",
        "*   **Lines 1-5:** This is the cell heading and docstring, outlining the cell's purpose and the metrics calculated.\n",
        "*   **Lines 7-8:** These lines import the `re` module for text cleaning and `Counter` from `collections` (though `Counter` isn't strictly needed for the current set logic, it's often useful for more complex token matching). It also assumes the necessary dictionaries containing the test questions, ground truth answers, and the generated answers (with and without RAG) are available from previous cells.\n",
        "*   **Lines 10-15:** A helper function `clean_answer` is defined. This function takes a string answer, converts it to lowercase, removes punctuation using a regular expression, and strips leading/trailing whitespace. This helps standardize the answers for comparison.\n",
        "*   **Lines 17-58:** The `evaluate_scores` function is defined. This function takes the generated answers dictionary and the ground truth answers dictionary as input and returns the calculated metrics.\n",
        "    *   Lines 20-26 initialize counters for accuracy and variables for summing true positives, predicted positives, and actual positives across all questions.\n",
        "    *   Lines 28-55 loop through each `question` in the `test_questions` list.\n",
        "    *   Lines 29-30 retrieve and clean the generated and ground truth answers.\n",
        "    *   Lines 32-33 split the cleaned answers into sets of keywords.\n",
        "    *   Lines 36-37 implement the *basic accuracy check* as before: checking if any keyword from the truth (longer than 2 characters) is present in the generated keywords.\n",
        "    *   Lines 40-52 calculate components for a *keyword-based* precision and recall.\n",
        "        *   `true_positives` (Line 43) is the count of keywords that are present in *both* the ground truth and the generated answer.\n",
        "        *   `predicted_positives` (Line 48) is simplified in this basic implementation to be equal to `true_positives`. In a more standard text generation evaluation, this would typically be the total number of relevant words or phrases in the generated answer.\n",
        "        *   `actual_positives` (Line 51) is the total number of keywords in the ground truth answer.\n",
        "        *   Lines 54-56 sum these counts across all questions.\n",
        "    *   Lines 60-62 calculate the overall accuracy percentage.\n",
        "    *   Lines 64-71 calculate the overall precision, recall, and F1-score based on the summed counts, including checks to avoid division by zero.\n",
        "    *   Line 73 returns the calculated accuracy, precision, recall, and F1-score.\n",
        "*   **Lines 75-81:** The `evaluate_scores` function is called to calculate the metrics for the `answers_without_rag`, and the results are printed with a clear heading.\n",
        "*   **Lines 84-90:** The `evaluate_scores` function is called again to calculate the metrics for the `answers_with_rag`, and the results are printed with a clear heading.\n",
        "*   **Lines 93-96:** A markdown note is included to reiterate that more sophisticated evaluation methods are recommended for a capstone and that the current precision/recall/F1 are based on simple keyword overlap, not standard text generation metrics.\n",
        "\n",
        "This step provides a preliminary comparison using basic accuracy, precision, and F1-score based on keyword matching. While useful for a quick look, it's important to remember the limitations of this method for evaluating the nuanced quality of generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5411193d"
      },
      "source": [
        "# %% EVALUATION STEP 4 ‚Äì Evaluate Answers and Calculate Scores\n",
        "\"\"\"\n",
        "This cell compares the generated answers (with and without RAG) to the ground truth\n",
        "answers and calculates basic evaluation scores like accuracy, precision, and F1-score.\n",
        "\"\"\"\n",
        "\n",
        "# Assuming 'test_questions', 'ground_truth_answers', 'answers_without_rag',\n",
        "# and 'answers_with_rag' are defined in previous cells\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def clean_answer(answer):\n",
        "    \"\"\"Basic cleaning of generated answers for comparison.\"\"\"\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    answer = answer.lower()\n",
        "    answer = re.sub(r'[^\\w\\s]', '', answer)\n",
        "    return answer.strip()\n",
        "\n",
        "def evaluate_scores(generated_answers, ground_truth):\n",
        "    \"\"\"Calculates basic accuracy, precision, and F1-score based on keyword presence.\"\"\"\n",
        "    correct_count = 0\n",
        "    total_count = len(test_questions)\n",
        "\n",
        "    # Variables for precision and recall calculation across all questions\n",
        "    total_true_positives = 0\n",
        "    total_predicted_positives = 0\n",
        "    total_actual_positives = 0\n",
        "\n",
        "    for question in test_questions:\n",
        "        generated = clean_answer(generated_answers.get(question, \"\"))\n",
        "        truth = clean_answer(ground_truth.get(question, \"\"))\n",
        "\n",
        "        truth_keywords = set(truth.split())\n",
        "        generated_keywords = set(generated.split())\n",
        "\n",
        "        # Basic Accuracy Check (as before)\n",
        "        if any(keyword in generated_keywords for keyword in truth_keywords if len(keyword) > 2):\n",
        "             correct_count += 1\n",
        "\n",
        "        # Keyword-based Precision and Recall (simplified for this basic metric)\n",
        "        # True Positives: Keywords from truth that are also in generated\n",
        "        true_positives = len(truth_keywords.intersection(generated_keywords))\n",
        "\n",
        "        # Predicted Positives: Keywords in generated (that are also in truth - for this definition)\n",
        "        # Using the number of true positives as predicted positives here simplifies\n",
        "        # the metric based on our current keyword-matching logic.\n",
        "        predicted_positives = true_positives # Simplified: Only count keywords from truth that were found\n",
        "\n",
        "        # Actual Positives: Keywords in the ground truth\n",
        "        actual_positives = len(truth_keywords)\n",
        "\n",
        "        total_true_positives += true_positives\n",
        "        total_predicted_positives += predicted_positives # Summing simplified predicted positives\n",
        "        total_actual_positives += actual_positives\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "    # Avoid division by zero\n",
        "    precision = total_true_positives / total_predicted_positives if total_predicted_positives > 0 else 0.0\n",
        "    recall = total_true_positives / total_actual_positives if total_actual_positives > 0 else 0.0\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Calculate scores for answers without RAG\n",
        "accuracy_without_rag, precision_without_rag, recall_without_rag, f1_without_rag = evaluate_scores(answers_without_rag, ground_truth_answers)\n",
        "print(f\"--- Scores Without RAG ---\")\n",
        "print(f\"Accuracy: {accuracy_without_rag:.2f}%\")\n",
        "print(f\"Precision: {precision_without_rag:.2f}\")\n",
        "print(f\"Recall: {recall_without_rag:.2f}\")\n",
        "print(f\"F1 Score: {f1_without_rag:.2f}\\n\")\n",
        "\n",
        "\n",
        "# Calculate scores for answers with RAG\n",
        "accuracy_with_rag, precision_with_rag, recall_with_rag, f1_with_rag = evaluate_scores(answers_with_rag, ground_truth_answers)\n",
        "print(f\"--- Scores With RAG ---\")\n",
        "print(f\"Accuracy: {accuracy_with_rag:.2f}%\")\n",
        "print(f\"Precision: {precision_with_rag:.2f}\")\n",
        "print(f\"Recall: {recall_with_rag:.2f}\")\n",
        "print(f\"F1 Score: {f1_with_rag:.2f}\")\n",
        "\n",
        "\n",
        "# Note: For a capstone, more sophisticated evaluation metrics (e.g., ROUGE, BERTScore)\n",
        "# and potentially human evaluation would provide a more complete picture.\n",
        "# The current precision/recall/f1 here are based on simple keyword overlap,\n",
        "# not standard metrics for text generation quality."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b2fb20"
      },
      "source": [
        "## Summary of RAG Showcase and Evaluation\n",
        "\n",
        "Based on the output of the RAG showcase (Cell 7) and the evaluation results (Evaluation Step 4) in the notebook, here is a summary:\n",
        "\n",
        "**RAG Showcase (Cell 7):**\n",
        "\n",
        "The showcase demonstrated the RAG system by answering five specific questions about diabetes guidelines. For each question, the system retrieved relevant knowledge graph triples from the FAISS index and then used the DeepSeek-7B model to generate an answer based on the retrieved context. The output shows the question, the generated answer, and the top-k retrieved triples with their scores.\n",
        "\n",
        "For example:\n",
        "*   For the question \"Which drug treats type 2 diabetes?\", the system retrieved triples like \"Insulin analogues Drug\\_Disease Type 2 diabetes\" and \"insulin Drug\\_Disease Type 2 diabetes\" and the model answered \"Metformin\".\n",
        "*   For \"What long-term complication of diabetes affects the eyes?\", retrieved triples included those related to \"Retina\" and \"Diabetic retinopathy\", leading to the answer \"Nonproliferative diabetic retinopathy.\"\n",
        "\n",
        "The showcase visually confirms that the retrieval component is finding relevant triples and the language model is using this context to generate answers.\n",
        "\n",
        "**Evaluation Results (Evaluation Step 4):**\n",
        "\n",
        "The evaluation compared the answers generated by the DeepSeek-7B model *without* RAG (using only its internal knowledge) and *with* RAG (using retrieved triples) against a set of ground truth answers based on keyword overlap. The results are as follows:\n",
        "\n",
        "*   **Scores Without RAG:**\n",
        "    *   Accuracy: 90.00%\n",
        "    *   Precision: 1.00\n",
        "    *   Recall: 0.46\n",
        "    *   F1 Score: 0.63\n",
        "\n",
        "*   **Scores With RAG:**\n",
        "    *   Accuracy: 80.00%\n",
        "    *   Precision: 1.00\n",
        "    *   Recall: 0.31\n",
        "    *   F1 Score: 0.48\n",
        "\n",
        "**Summary of Findings:**\n",
        "\n",
        "Interestingly, based on the simple keyword-overlap evaluation metrics used here, the model *without* RAG achieved slightly higher Accuracy, Recall, and F1 scores compared to the model *with* RAG. Precision was 1.00 for both, which in this simplified keyword-based metric means that any keyword from the ground truth found in the generated answer was counted as a true positive (and predicted positive).\n",
        "\n",
        "This initial evaluation suggests that for this specific set of questions and the current configuration:\n",
        "*   The base DeepSeek-7B model has significant pre-trained knowledge about diabetes.\n",
        "*   The RAG system, in this instance and with this evaluation method, did not demonstrably improve performance based on these metrics. This could be due to various factors, including:\n",
        "    *   The quality or relevance of the retrieved triples for these specific questions.\n",
        "    *   How effectively the DeepSeek-7B model is utilizing the provided context block.\n",
        "    *   The limitations of the simple keyword-overlap evaluation metrics in capturing the full quality and nuance of the generated answers.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "We successfully refactored the translation process, resolved persistent library dependency issues, built the FAISS index, and ran the RAG pipeline and evaluation. While the RAG showcase visually confirms that context is being retrieved and used, the initial keyword-based evaluation metrics do not show an improvement over the base model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d641bbd1"
      },
      "source": [
        "### Note on Evaluation Metrics and Sample Size\n",
        "\n",
        "It's important to note the following regarding the evaluation results presented:\n",
        "\n",
        "*   **Sample Size:** The evaluation was conducted using a set of **10 test questions** (as defined in Evaluation Step 1). This is a relatively small sample size for a comprehensive evaluation of an LLM or RAG system.\n",
        "*   **Evaluation Metric:** The metric used (in Evaluation Step 4) is a **simple keyword-based overlap** comparison between the generated answers and the ground truth answers. While this provides a basic measure of content presence, it has limitations. It does not fully capture semantic similarity, fluency, correctness of phrasing, or the overall quality of the generated text in the way more sophisticated metrics like ROUGE or BERTScore do.\n",
        "*   **Conclusion Limitations:** Due to the small sample size and the nature of the keyword-based metric, the evaluation scores obtained should be considered **preliminary** and **not conclusive**. They provide an initial indication that the RAG system, in this specific setup, did not show a significant improvement over the base model based on this metric. For a robust analysis, a larger test set and more advanced evaluation methods would be necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTfw8UksyQId"
      },
      "source": [
        "---\n",
        "# APPENDIX AND EXPLORATORY CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VOvrMFEye2s"
      },
      "source": [
        "# %% APPENDIX A1 ‚Äì Analyze JSON Structure and Counts\n",
        "\"\"\"\n",
        "This cell analyzes the structure of the translated JSON files and counts the number\n",
        "of unique entity types, relationship types, and total triples. This information\n",
        "is crucial for understanding the content and organization of the knowledge graph\n",
        "data, which can be very helpful for discussing the project during capstone\n",
        "research meetings.\n",
        "\"\"\"\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Assuming JSON_DIR is defined in a previous cell and points to our translated JSON files\n",
        "# Example path, adjust if necessary based on our notebook's CELL 0\n",
        "# JSON_DIR = Path(\"/content/drive/MyDrive/diakg_assets/diakg_en\")\n",
        "\n",
        "# Get a list of all translated JSON files\n",
        "json_files = list(JSON_DIR.glob(\"*_en.json\"))\n",
        "\n",
        "if json_files:\n",
        "    # Load one sample JSON file to inspect its structure\n",
        "    sample_file = json_files[0]\n",
        "    print(f\"Analyzing sample file: {sample_file.name}\")\n",
        "\n",
        "    with open(sample_file, 'r', encoding='utf8') as f:\n",
        "        sample_data = json.load(f)\n",
        "\n",
        "    # Inspect the top-level keys\n",
        "    print(\"\\nTop-level keys in a sample JSON file:\")\n",
        "    print(sample_data.keys())\n",
        "\n",
        "    # Inspect the structure within a paragraph and sentence\n",
        "    if \"paragraphs\" in sample_data and sample_data[\"paragraphs\"]:\n",
        "        sample_paragraph = sample_data[\"paragraphs\"][0]\n",
        "        print(\"\\nKeys in a sample paragraph:\")\n",
        "        print(sample_paragraph.keys())\n",
        "\n",
        "        if \"sentences\" in sample_paragraph and sample_paragraph[\"sentences\"]:\n",
        "            sample_sentence = sample_paragraph[\"sentences\"][0]\n",
        "            print(\"\\nKeys in a sample sentence:\")\n",
        "            print(sample_sentence.keys())\n",
        "\n",
        "            # Count entity types, relationship types, and total triples across all files\n",
        "            entity_types = set()\n",
        "            relation_types = set()\n",
        "            total_triples = 0\n",
        "\n",
        "            print(\"\\nCounting entity types, relationship types, and triples across all files...\")\n",
        "            for json_file in json_files:\n",
        "                with open(json_file, 'r', encoding='utf8') as f:\n",
        "                    data = json.load(f)\n",
        "                    if \"paragraphs\" in data:\n",
        "                        for paragraph in data[\"paragraphs\"]:\n",
        "                            if \"sentences\" in paragraph:\n",
        "                                for sentence in paragraph[\"sentences\"]:\n",
        "                                    if \"entities\" in sentence:\n",
        "                                        for entity in sentence[\"entities\"]:\n",
        "                                            # Corrected: Assuming 'entity_type' key exists for entity type\n",
        "                                            if 'entity_type' in entity:\n",
        "                                                entity_types.add(entity['entity_type'])\n",
        "                                    if \"relations\" in sentence:\n",
        "                                        for relation in sentence[\"relations\"]:\n",
        "                                            # Assuming 'relation_type' key exists for relationship type\n",
        "                                            if 'relation_type' in relation:\n",
        "                                                relation_types.add(relation['relation_type'])\n",
        "                                            # Each relation is considered a triple\n",
        "                                            total_triples += 1\n",
        "\n",
        "            print(f\"\\nTotal number of unique entity types: {len(entity_types)}\")\n",
        "            print(f\"Total number of unique relationship types: {len(relation_types)}\")\n",
        "            print(f\"Total number of triples (relations): {total_triples}\")\n",
        "\n",
        "            print(\"\\nSample Entity Types:\")\n",
        "            print(list(entity_types)[:10]) # Print first 10 for brevity\n",
        "\n",
        "            print(\"\\nSample Relationship Types:\")\n",
        "            print(list(relation_types)[:10]) # Print first 10 for brevity\n",
        "\n",
        "else:\n",
        "    print(f\"No translated JSON files found in {JSON_DIR}. Please run the translation cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAnwGw0myuVb"
      },
      "source": [
        "## APPENDIX A2 ‚Äì Explanation of JSON Structure Analysis\n",
        "\n",
        "This markdown cell explains the purpose, functionality, and benefits of the preceding code cell (APPENDIX A1), which analyzes the structure and content of the translated Chinese diabetes guideline JSON files.\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "The code cell performs two main tasks:\n",
        "1. **Inspects the structure of a sample JSON file:** It loads the first translated JSON file found in the specified directory and prints the top-level keys, as well as the keys within a sample paragraph and sentence. This provides a clear overview of how the data is organized hierarchically.\n",
        "2. **Counts entity types, relationship types, and triples:** It iterates through *all* the translated JSON files to identify and count the unique types of entities and relationships present in the dataset. It also counts the total number of relations, which represent the knowledge graph triples.\n",
        "\n",
        "**How it does it:**\n",
        "\n",
        "*   It uses Python's built-in `json` library to load and parse the JSON files.\n",
        "*   The `pathlib` module is used for easy handling of file paths.\n",
        "*   It accesses nested dictionaries and lists within the JSON structure to find entity and relation information.\n",
        "*   `set()` is used to efficiently store and count unique entity and relationship types.\n",
        "*   A loop iterates through all the JSON files to aggregate the counts.\n",
        "\n",
        "**Why it's helpful:**\n",
        "\n",
        "For the capstone research project, understanding the underlying data is essential. This analysis provides concrete numbers and structural insights that we can use to:\n",
        "\n",
        "*   **Describe the dataset:** Quantify the diversity of information by stating the number of entity and relationship types.\n",
        "*   **Explain the data source:** Clearly articulate the structure of the input data\n",
        "*   **Justify design choices:** Relate the structure of the JSON data to decisions made in building the knowledge graph and RAG system.\n",
        "*   **Estimate scale:** The total number of triples gives us an idea of the size of the knowledge graph being built.\n",
        "\n",
        "By having this information readily available, we can speak confidently and knowledgeably about the data underpinning the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "344a55ee"
      },
      "source": [
        "# More Refactoring Info\n",
        "Refactor the provided Jupyter notebook `/content/RichCapstoneFineTuningMistral.ipynb` to replace the current translation method with one using the `deep_translator` library, specifically Google Translate with MyMemory as a fallback, incorporating caching for efficiency. This involves analyzing the existing code, implementing the new translation logic, modifying the data processing steps to use the new translations, and testing the updated pipeline. Also, back up the `/content/drive/MyDrive/diakg_assets` folder before making changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a2cd3e"
      },
      "source": [
        "## Backup of existing data due to refactoring and forking the code\n",
        "\n",
        "### Subtask:\n",
        "Manually rename the `/content/drive/MyDrive/diakg_assets` folder in the Google Drive to `/content/drive/MyDrive/diakg_assets.old`. Then, make a copy of the contents from `/content/drive/MyDrive/diakg_assets.old` into a new folder named `/content/drive/MyDrive/diakg_assets`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88e405d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a new code cell to install the deep_translator library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd4b8046"
      },
      "source": [
        "* * *\n",
        "## Retrospective and Lessons Learned\n",
        "\n",
        "This project involved building a complete RAG pipeline, encountering and solving several challenges along the way. The process highlighted key aspects of working with large language models, knowledge graphs, and cloud environments.\n",
        "\n",
        "**Lessons Learned:**\n",
        "\n",
        "*   **Dependency Management is Crucial and Complex:** Pinning library versions is essential for reproducibility and stability, especially in dynamic environments like Colab. However, finding a truly compatible set of dependencies across multiple libraries (like `transformers`, `sentence-transformers`, `peft`, `accelerate`, `bitsandbytes`, `huggingface_hub`) can be extremely challenging and time-consuming.\n",
        "*   **Persistent Errors Require Systematic Debugging:** The `ModuleNotFoundError` in Cell 4 demonstrated that simple reinstallations are not always sufficient. Debugging required systematically trying different version combinations, understanding the traceback, and considering interactions between multiple libraries.\n",
        "*   **Runtime Restarts Can Solve Stubborn Issues:** For persistent environment-related errors in Colab, a runtime restart is a valuable troubleshooting step that can clear cached states that `%pip install --upgrade` might miss.\n",
        "*   **Translating Structured Data Requires Careful Planning:** When translating data with complex structures (like JSON with nested entities and relations), it's vital to consider which fields need translation and at what granularity, based on the downstream use of the data (e.g., building knowledge graph triples).\n",
        "*   **Robust Translation Handling is Necessary for External APIs:** Relying on external translation APIs requires implementing robust error handling, including retry mechanisms (like exponential backoff) and fallbacks, to deal with rate limits, server errors, and temporary service unavailability.\n",
        "*   **Caching Improves Efficiency with Repeated Data:** For translation tasks, implementing a cache for frequently encountered text snippets significantly reduces redundant API calls and speeds up the process.\n",
        "*   **Parallel Processing Accelerates File-Based Operations:** Utilizing tools like `ThreadPoolExecutor` for tasks involving processing multiple files concurrently (like translation) is crucial for reducing overall execution time.\n",
        "*   **Clear Notebook Structure and Documentation Aid Understanding:** Organizing the notebook into logical cells with clear markdown explanations and comments significantly improves readability and allows others (and myself) to understand the pipeline and reproduce the results.\n",
        "*   **Evaluation Metrics Matter and Have Limitations:** Choosing appropriate evaluation metrics is key to assessing model performance. Simple metrics like keyword overlap can provide a quick overview but have limitations in capturing semantic meaning and fluency. More sophisticated metrics (ROUGE, BERTScore, human evaluation) are needed for a comprehensive assessment.\n",
        "*   **RAG Performance Depends on Retrieval Quality and Model Utilization:** The initial evaluation not showing RAG improvement highlights that effective RAG relies on both retrieving truly relevant context *and* the language model effectively using that context rather than relying solely on its internal knowledge.\n",
        "*   **Debugging Takes Time and Patience:** Resolving complex technical issues, especially dependency problems in machine learning environments, often requires significant time, experimentation, and patience.\n",
        "*   **Understanding Error Messages is Key:** Carefully reading and interpreting traceback messages is fundamental to identifying the root cause of errors and guiding troubleshooting efforts.\n",
        "*   **Iterative Development is Effective:** Breaking down the project into smaller, manageable steps (like translation, indexing, retrieval, evaluation) and addressing issues at each stage is an effective development strategy.\n",
        "*   **Community Resources and Documentation are Valuable:** Consulting documentation for libraries like `transformers`, `sentence-transformers`, and `deep_translator`, and looking at community forums for similar errors, can provide crucial clues and solutions.\n",
        "*   **Version Compatibility Tools (Less Used Here, But Relevant):** While not extensively used in my interactive debugging, tools like `pipdeptree` or creating isolated environments (like with `venv` or Conda) can sometimes help diagnose and manage complex dependencies more proactively."
      ]
    }
  ]
}